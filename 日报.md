2022-3-15

1. EET适配CLIP模型

   可以灵活适配多种text和vision模型，拷贝weight的dict适配
   
   关于EET Model的使用和拼接，借助from_torch方法
   
   适配：1、transformers的预训练模型；2、任意的text model和vision model
   
   

2022年3月23日

1. EET重构：with no_grad 性能比较

2. CLIP精度问题：

   [ 0.933, -0.669,  0.275,  ..., -0.853, -0.218, -0.261]

   [ 0.485, -0.605,  0.621,  ..., -0.659,  0.035, -0.799]

   eet

   [ 0.606, -0.683,  0.496,  ..., -0.541,  0.087, -0.726]

   gelu 激活函数 quick_gelu

   attention mask和casual attention mask

   layer id:  0  ts encoder output:  tensor([[[-0.037, -0.057, -0.077,  ...,  0.120, -0.148, -0.017]

   layer id:  0  eet encoder output:  tensor([[[-0.131, -0.084, -0.102,  ...,  0.220, -0.176,  0.042]
   
3. Transformers CLIP不支持fp16推理，需要修改源码。（因为text model的causal_attention_mask为float32）



2022年4月1日

1. csrc/core/bert_softmax.cu

   性能优化：softmax kernel计算（1）mask直接将输出值置为0，省去scale softmax等计算
   
2. huggingface transformers的clip模型半精度需要修改build_causal_mask函数

3. 进一步优化：**激活函数if语句条件合并；qk_softmax计算简化**

4. **适配albert，gpt模型**，进一步简化加载模型部分代码



2022年4月7日

1. T5模型适配（先往后
2. 继续优化mapping、model loading和底层实现
3. **开发精度对比工具**：输入子模型和输入向量
4. EET CLIP精度随着seq_len增大而降低
5. **device id由用户输入**
6. encoder，decoder，layernorm需要加一个**is_bias的配置项**，
7. 新增依赖项：django
8. 增加**注释**



2022年4月13日

1. 注释，适配albert和gpt；albert实现了group的逻辑
2. 进一步重构：model api使用方法from_pretrained structure mapping加载预训练模型参数，op api使用from torch方法加载指定模型参数



2022年4月22日

1. 适配难点：qkv不统一；Conv1D和Linear weight转置不同
2. 精度对齐问题
3. bug：一块缓存需要常驻（静态内存管理）不可写
4. bart模型的embedding改成eet的

5. embedding **fp16bug**：encoder-decoder结构模型中遇到的问题：fp16情况下，embedding存在精度问题（可能也和缓存有关），判断是内存排布问题，不易定位——猜测：fp16申请的get_buffer内存被踩（打印内存）



2022年5月6日

1. T5模型、Bart模型，transformer模型
2. **fp16 bug**：猜测：deepcopy在fp16的情况下踩了内存；eet embedding性能测试bs=4，seq_len=64场景fp16（3.45->3.6)，fp32(1.86->1.90) ?性能波动
3. 1375MB，**GPT模型去掉clone**



2022年5月18日

1. 1080ti cuda 11.1升级到11.4，性能无提升
2. softmax优化



2022年5月20日

1. gemm择优：https://gitlab.leihuo.netease.com/ligongzheng/eet/-/blob/master/Python/src/bert_encoding.h	setGenConfig loadConfig genConfig
2. cross_attention问题定位
3. relative_pos_bias：优化点和softmax融合







|          operators          |       python API       |                  Remarks                  |
| :-------------------------: | :--------------------: | :---------------------------------------: |
|    multi_head_attention     |    EETSelfAttention    |              self attention               |
| masked_multi_head_attention | EETSelfMaskedAttention |             causal attention              |
| cross_multi_head_attention  |   EETCrossAttention    |              cross attention              |
|             ffn             |     EETFeedforward     |           feed forward network            |
|          embedding          |    EETBertEmbedding    | correspondence to Fairseq and Transfomers |
|          layernorm          |      EETLayerNorm      |           same as nn.LayerNorm            |






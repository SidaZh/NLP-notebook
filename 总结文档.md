## 1. 工作环境及内容

**设备：**

pc 2060

服务器 3090



**丹炉云平台**
使用方法：

1. 配置个人存储

2. 开发环境

   申请资源：6核，30GB内存，3090/镜像danlu-modelserving/eet:v20.12

   （1）服务器IP：ai.danlu.netease.com 端口号：xxx

   （2）下载密钥和公钥，

   （3）使用私钥ssh连接远程服务器

   服务器环境配置：

   （1）conda环境激活，其他必备环境镜像已准备好
   
   （2）eet运行python包依赖：pip install transformers fairseq django -i https://pypi.douban.com/simple/
   
3. 训练环境

   启动命令：/etc/init.d/ssh start && tail -f /root/1.txt

   

   （1）vim中文乱码问题：/etc/vim/vimrc 添加set encoding=utf-8

   （2）history使用page up down搜索命令：vi /etc/inputrc "\e[5~": history-search-backward "\e[6~": history-search-forward



**代码管理：**

1. gitlab开发
2. github EET框架



**轮子：**

streamlit



#### 1.1 新员工入职

- 常用软件：
    - 开发环境：
      云平台：丹炉，可ssh连接服务器
      本地看代码：vscode，pycharm

    - 办公软件

      **visual code：**

      - Shift + Alt + F：自动排版
      - Ctrl+K Ctrl+F：选定内容自动排版
      - 

    ```
    ssh远程开发
    
    1、python代码跳转问题：settings.json中加"python.languageServer":"Jedi"
    2、无法打开源文件的问题：C++ configuration增加包含路径
    ${workspaceFolder}/**
    /usr/local/cuda-11.1/**
    /opt/conda/lib/python3.8/site-packages/**
    ```



- 常用命令

  watch -n 2 每隔2s监视一次运行结果



## 2. EET框架

#### 2.1 基本使用

EETBertModel

​	__init__

​	__call__

​		input_ids:输入序列在词汇表中的索引

init：构造函数，使用加载预训练模型param的模型模块

call：调用前向过程

from_torch：加载torch的tensor（weight），再调用init函数



from_pretrained

layer_model_dict：	dict{分组键（网络结构名前8位）：dict{state_dict网络结构名称：对应参数tensor}}



- **tips：**
  1. GPU第一次推理速度很慢（与初始化有关），——但是EET框架算子不存在这个问题
  2. nvprof性能分析工具，
  3. NVIDIA Nsight分析工具





#### 2.2 体会

1. EET模型做微调，适配权重参数；——易用性





#### 2.3 性能优化

1. Gemm矩阵乘，cublas库，gemm择优
2. 混合精度，int8量化
3. 



​	











## 3. huggingface模型

**Bert**

Embedding

embeddings.position_ids
embeddings.word_embeddings.weight
embeddings.position_embeddings.weight
embeddings.token_type_embeddings.weight
embeddings.LayerNorm.weight
embeddings.LayerNorm.bias

encoder

QKV + Wo + LN + FFN + LN

layer.0.attention.self.query.weight
layer.0.attention.self.query.bias
layer.0.attention.self.key.weight
layer.0.attention.self.key.bias
layer.0.attention.self.value.weight
layer.0.attention.self.value.bias
layer.0.attention.output.dense.weight
layer.0.attention.output.dense.bias
layer.0.attention.output.LayerNorm.weight
layer.0.attention.output.LayerNorm.bias
layer.0.intermediate.dense.weight
layer.0.intermediate.dense.bias
layer.0.output.dense.weight
layer.0.output.dense.bias
layer.0.output.LayerNorm.weight
layer.0.output.LayerNorm.bias



**ViT**

Patch_Embedding

\# in ViT, layernorm is applied before self-attention





## 4. Python

for k, g in groupby(data, keyfunc)：

data是分组数据（可迭代对象），keyfunc是分组函数

k是当前分组键，g是一个迭代器，可用于在该分组键定义的组上迭代。换句话说，groupby迭代器本身返回迭代器。




$$
\sqrt{k}
$$





## 5. CUDA编程

#### 5.1 编程模型

线程块

根据线程ID寻址：对于 一个三维的大小为 (Dx，Dy，Dz)的块，这个线程的索引是(x，y，z)， 线程的ID 是(x + y Dx + z DxDy)。

```text
threadIdx.x
threadIdx.y
blockIdx.x
blockIdx.y
```

线程的内置变量blockDim，获取线程块各个维度的大小（Dx,Dy)

```
blockDim.x,y,z	给出块中特定方向的线程数(长宽高)
gridDim.x,y,z	给出网格中特定方向的块数
```

![img](https://images2015.cnblogs.com/blog/494924/201703/494924-20170303224153626-1838869747.png)



![img](https://images2015.cnblogs.com/blog/494924/201703/494924-20170303224323438-2036415075.png)

block 2D 3*4

| (0,0) | (1,0) | (2,0) |
| ----- | ----- | ----- |
| (0,1) | (1,1) | (2,1) |
| (0,2) | (1,2) | (2,2) |
| (0,3) | (1,3) | (2,3) |



| 0    | 1    | 2    |
| ---- | ---- | ---- |
| 3    | 4    | 5    |
| 6    | 7    | 8    |
| 9    | 10   | 11   |



    int blockId = blockIdx.x + blockIdx.y * gridDim.x  
                     + gridDim.x * gridDim.y * blockIdx.z;  
    int threadId = blockId * (blockDim.x * blockDim.y * blockDim.z)  
                       + (threadIdx.z * (blockDim.x * blockDim.y))  
                       + (threadIdx.y * blockDim.x) + threadIdx.x;   


#### 5.2 kernel代码

template模板函数需要显式实例化

确定index：（1）index的范围；（2）作为索引时的映射或对应关系



#### 5.3 cuda fp16

CUDA 7.5 中定义的 half2 结构在一个32位的字中存储了两个半精度浮点数

![image-20220506141250035](C:\Users\zhaosida\AppData\Roaming\Typora\typora-user-images\image-20220506141250035.png)



#### 5.4 cublas库









## 6. NLP

softmax

layernorm

batchnorm